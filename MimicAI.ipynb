{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Mel-filterbank\n",
    "mel_window_length = 25  # In milliseconds\n",
    "mel_window_step = 10    # In milliseconds\n",
    "mel_n_channels = 40\n",
    "\n",
    "\n",
    "## Audio\n",
    "sampling_rate = 16000\n",
    "# Number of spectrogram frames in a partial utterance\n",
    "partials_n_frames = 160     # 1600 ms\n",
    "# Number of spectrogram frames at inference\n",
    "inference_n_frames = 80     #  800 ms\n",
    "\n",
    "\n",
    "## Voice Activation Detection\n",
    "# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n",
    "# This sets the granularity of the VAD. Should not need to be changed.\n",
    "vad_window_length = 30  # In milliseconds\n",
    "# Number of frames to average together when performing the moving average smoothing.\n",
    "# The larger this value, the larger the VAD variations must be to not get smoothed out. \n",
    "vad_moving_average_width = 8\n",
    "# Maximum number of consecutive silent frames a segment can have.\n",
    "vad_max_silence_length = 6\n",
    "\n",
    "\n",
    "## Audio volume normalization\n",
    "audio_norm_target_dBFS = -30\n",
    "\n",
    "\n",
    "## Model parameters\n",
    "model_hidden_size = 256\n",
    "model_embedding_size = 256\n",
    "model_num_layers = 3\n",
    "\n",
    "\n",
    "## Training parameters\n",
    "learning_rate_init = 1e-4\n",
    "speakers_per_batch = 64\n",
    "utterances_per_speaker = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from encoder.params_model import *\n",
    "# from encoder.params_data import *\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from scipy.optimize import brentq\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(self, device, loss_device):\n",
    "        super().__init__()\n",
    "        self.loss_device = loss_device\n",
    "        \n",
    "        # Network defition\n",
    "        self.lstm = nn.LSTM(input_size=mel_n_channels,\n",
    "                            hidden_size=model_hidden_size, \n",
    "                            num_layers=model_num_layers, \n",
    "                            batch_first=True).to(device)\n",
    "        self.linear = nn.Linear(in_features=model_hidden_size, \n",
    "                                out_features=model_embedding_size).to(device)\n",
    "        self.relu = torch.nn.ReLU().to(device)\n",
    "        \n",
    "        # Cosine similarity scaling (with fixed initial parameter values)\n",
    "        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(loss_device)\n",
    "        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(loss_device)\n",
    "\n",
    "        # Loss\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(loss_device)\n",
    "        \n",
    "    def do_gradient_ops(self):\n",
    "        # Gradient scale\n",
    "        self.similarity_weight.grad *= 0.01\n",
    "        self.similarity_bias.grad *= 0.01\n",
    "            \n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n",
    "    \n",
    "    def forward(self, utterances, hidden_init=None):\n",
    "        \"\"\"\n",
    "        Computes the embeddings of a batch of utterance spectrograms.\n",
    "        \n",
    "        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape \n",
    "        (batch_size, n_frames, n_channels) \n",
    "        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers, \n",
    "        batch_size, hidden_size). Will default to a tensor of zeros if None.\n",
    "        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n",
    "        # and the final cell state.\n",
    "        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n",
    "        \n",
    "        # We take only the hidden state of the last layer\n",
    "        embeds_raw = self.relu(self.linear(hidden[-1]))\n",
    "        \n",
    "        # L2-normalize it\n",
    "        embeds = embeds_raw / (torch.norm(embeds_raw, dim=1, keepdim=True) + 1e-5)        \n",
    "\n",
    "        return embeds\n",
    "    \n",
    "    def similarity_matrix(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the similarity matrix according the section 2.1 of GE2E.\n",
    "\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, speakers_per_batch)\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "        \n",
    "        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "        centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "        # Exclusive centroids (1 per utterance)\n",
    "        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "        centroids_excl /= (utterances_per_speaker - 1)\n",
    "        centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "        # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n",
    "        # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n",
    "        # We vectorize the computation for efficiency.\n",
    "        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                 speakers_per_batch).to(self.loss_device)\n",
    "        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n",
    "        for j in range(speakers_per_batch):\n",
    "            mask = np.where(mask_matrix[j])[0]\n",
    "            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "        \n",
    "        ## Even more vectorized version (slower maybe because of transpose)\n",
    "        # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n",
    "        #                           ).to(self.loss_device)\n",
    "        # eye = np.eye(speakers_per_batch, dtype=np.int)\n",
    "        # mask = np.where(1 - eye)\n",
    "        # sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2)\n",
    "        # mask = np.where(eye)\n",
    "        # sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2)\n",
    "        # sim_matrix2 = sim_matrix2.transpose(1, 2)\n",
    "        \n",
    "        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n",
    "        return sim_matrix\n",
    "    \n",
    "    def loss(self, embeds):\n",
    "        \"\"\"\n",
    "        Computes the softmax loss according the section 2.1 of GE2E.\n",
    "        \n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the loss and the EER for this batch of embeddings.\n",
    "        \"\"\"\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "        \n",
    "        # Loss\n",
    "        sim_matrix = self.similarity_matrix(embeds)\n",
    "        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, \n",
    "                                         speakers_per_batch))\n",
    "        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n",
    "        loss = self.loss_fn(sim_matrix, target)\n",
    "        \n",
    "        # EER (not backpropagated)\n",
    "        with torch.no_grad():\n",
    "            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]\n",
    "            labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "            preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "            # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n",
    "            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "            \n",
    "        return loss, eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.morphology import binary_dilation\n",
    "# from encoder.params_data import *\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "import librosa\n",
    "import struct\n",
    "\n",
    "try:\n",
    "    import webrtcvad\n",
    "except:\n",
    "    warn(\"Unable to import 'webrtcvad'. This package enables noise removal and is recommended.\")\n",
    "    webrtcvad=None\n",
    "\n",
    "int16_max = (2 ** 15) - 1\n",
    "\n",
    "\n",
    "def Epreprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],\n",
    "                   source_sr: Optional[int] = None,\n",
    "                   normalize: Optional[bool] = True,\n",
    "                   trim_silence: Optional[bool] = True):\n",
    "    \"\"\"\n",
    "    Applies the preprocessing operations used in training the Speaker Encoder to a waveform \n",
    "    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n",
    "\n",
    "    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not \n",
    "    just .wav), either the waveform as a numpy array of floats.\n",
    "    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before \n",
    "    preprocessing. After preprocessing, the waveform's sampling rate will match the data \n",
    "    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and \n",
    "    this argument will be ignored.\n",
    "    \"\"\"\n",
    "    # Load the wav from disk if needed\n",
    "    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n",
    "        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n",
    "    else:\n",
    "        wav = fpath_or_wav\n",
    "    \n",
    "    # Resample the wav if needed\n",
    "    if source_sr is not None and source_sr != sampling_rate:\n",
    "        wav = librosa.resample(wav, source_sr, sampling_rate)\n",
    "        \n",
    "    # Apply the preprocessing: normalize volume and shorten long silences \n",
    "    if normalize:\n",
    "        wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)\n",
    "    if webrtcvad and trim_silence:\n",
    "        wav = trim_long_silences(wav)\n",
    "    \n",
    "    return wav\n",
    "\n",
    "\n",
    "def wav_to_mel_spectrogram(wav):\n",
    "    \"\"\"\n",
    "    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n",
    "    Note: this not a log-mel spectrogram.\n",
    "    \"\"\"\n",
    "    frames = librosa.feature.melspectrogram(\n",
    "        wav,\n",
    "        sampling_rate,\n",
    "        n_fft=int(sampling_rate * mel_window_length / 1000),\n",
    "        hop_length=int(sampling_rate * mel_window_step / 1000),\n",
    "        n_mels=mel_n_channels\n",
    "    )\n",
    "    return frames.astype(np.float32).T\n",
    "\n",
    "\n",
    "def trim_long_silences(wav):\n",
    "    \"\"\"\n",
    "    Ensures that segments without voice in the waveform remain no longer than a \n",
    "    threshold determined by the VAD parameters in params.py.\n",
    "\n",
    "    :param wav: the raw waveform as a numpy array of floats \n",
    "    :return: the same waveform with silences trimmed away (length <= original wav length)\n",
    "    \"\"\"\n",
    "    # Compute the voice detection window size\n",
    "    samples_per_window = (vad_window_length * sampling_rate) // 1000\n",
    "    \n",
    "    # Trim the end of the audio to have a multiple of the window size\n",
    "    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n",
    "    \n",
    "    # Convert the float waveform to 16-bit mono PCM\n",
    "    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n",
    "    \n",
    "    # Perform voice activation detection\n",
    "    voice_flags = []\n",
    "    vad = webrtcvad.Vad(mode=3)\n",
    "    for window_start in range(0, len(wav), samples_per_window):\n",
    "        window_end = window_start + samples_per_window\n",
    "        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],\n",
    "                                         sample_rate=sampling_rate))\n",
    "    voice_flags = np.array(voice_flags)\n",
    "    \n",
    "    # Smooth the voice detection with a moving average\n",
    "    def moving_average(array, width):\n",
    "        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n",
    "        ret = np.cumsum(array_padded, dtype=float)\n",
    "        ret[width:] = ret[width:] - ret[:-width]\n",
    "        return ret[width - 1:] / width\n",
    "    \n",
    "    audio_mask = moving_average(voice_flags, vad_moving_average_width)\n",
    "    audio_mask = np.round(audio_mask).astype(np.bool)\n",
    "    \n",
    "    # Dilate the voiced regions\n",
    "    audio_mask = binary_dilation(audio_mask, np.ones(vad_max_silence_length + 1))\n",
    "    audio_mask = np.repeat(audio_mask, samples_per_window)\n",
    "    \n",
    "    return wav[audio_mask == True]\n",
    "\n",
    "\n",
    "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n",
    "    if increase_only and decrease_only:\n",
    "        raise ValueError(\"Both increase only and decrease only are set\")\n",
    "    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n",
    "    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n",
    "        return wav\n",
    "    return wav * (10 ** (dBFS_change / 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "\n",
    "# from encoder.params_data import *\n",
    "# from encoder.model import SpeakerEncoder\n",
    "# from encoder.audio import preprocess_wav   # We want to expose this function from here\n",
    "from matplotlib import cm\n",
    "# from encoder import audio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "_model = None # type: SpeakerEncoder\n",
    "_device = None # type: torch.device\n",
    "\n",
    "\n",
    "def Eload_model(weights_fpath: Path, device=None):\n",
    "    \"\"\"\n",
    "    Loads the model in memory. If this function is not explicitely called, it will be run on the\n",
    "    first call to embed_frames() with the default weights file.\n",
    "\n",
    "    :param weights_fpath: the path to saved model weights.\n",
    "    :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\"). The\n",
    "    model will be loaded and will run on this device. Outputs will however always be on the cpu.\n",
    "    If None, will default to your GPU if it\"s available, otherwise your CPU.\n",
    "    \"\"\"\n",
    "    # TODO: I think the slow loading of the encoder might have something to do with the device it\n",
    "    #   was saved on. Worth investigating.\n",
    "    global _model, _device\n",
    "    if device is None:\n",
    "        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif isinstance(device, str):\n",
    "        _device = torch.device(device)\n",
    "    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n",
    "    checkpoint = torch.load(weights_fpath, _device)\n",
    "    _model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    _model.eval()\n",
    "    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath.name, checkpoint[\"step\"]))\n",
    "\n",
    "\n",
    "def is_loaded():\n",
    "    return _model is not None\n",
    "\n",
    "\n",
    "def embed_frames_batch(frames_batch):\n",
    "    \"\"\"\n",
    "    Computes embeddings for a batch of mel spectrogram.\n",
    "\n",
    "    :param frames_batch: a batch mel of spectrogram as a numpy array of float32 of shape\n",
    "    (batch_size, n_frames, n_channels)\n",
    "    :return: the embeddings as a numpy array of float32 of shape (batch_size, model_embedding_size)\n",
    "    \"\"\"\n",
    "    if _model is None:\n",
    "        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n",
    "\n",
    "    frames = torch.from_numpy(frames_batch).to(_device)\n",
    "    embed = _model.forward(frames).detach().cpu().numpy()\n",
    "    return embed\n",
    "\n",
    "\n",
    "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n",
    "                           min_pad_coverage=0.75, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Computes where to split an utterance waveform and its corresponding mel spectrogram to obtain\n",
    "    partial utterances of <partial_utterance_n_frames> each. Both the waveform and the mel\n",
    "    spectrogram slices are returned, so as to make each partial utterance waveform correspond to\n",
    "    its spectrogram. This function assumes that the mel spectrogram parameters used are those\n",
    "    defined in params_data.py.\n",
    "\n",
    "    The returned ranges may be indexing further than the length of the waveform. It is\n",
    "    recommended that you pad the waveform with zeros up to wave_slices[-1].stop.\n",
    "\n",
    "    :param n_samples: the number of samples in the waveform\n",
    "    :param partial_utterance_n_frames: the number of mel spectrogram frames in each partial\n",
    "    utterance\n",
    "    :param min_pad_coverage: when reaching the last partial utterance, it may or may not have\n",
    "    enough frames. If at least <min_pad_coverage> of <partial_utterance_n_frames> are present,\n",
    "    then the last partial utterance will be considered, as if we padded the audio. Otherwise,\n",
    "    it will be discarded, as if we trimmed the audio. If there aren't enough frames for 1 partial\n",
    "    utterance, this parameter is ignored so that the function always returns at least 1 slice.\n",
    "    :param overlap: by how much the partial utterance should overlap. If set to 0, the partial\n",
    "    utterances are entirely disjoint.\n",
    "    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n",
    "    respectively the waveform and the mel spectrogram with these slices to obtain the partial\n",
    "    utterances.\n",
    "    \"\"\"\n",
    "    assert 0 <= overlap < 1\n",
    "    assert 0 < min_pad_coverage <= 1\n",
    "\n",
    "    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n",
    "    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n",
    "    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n",
    "\n",
    "    # Compute the slices\n",
    "    wav_slices, mel_slices = [], []\n",
    "    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n",
    "    for i in range(0, steps, frame_step):\n",
    "        mel_range = np.array([i, i + partial_utterance_n_frames])\n",
    "        wav_range = mel_range * samples_per_frame\n",
    "        mel_slices.append(slice(*mel_range))\n",
    "        wav_slices.append(slice(*wav_range))\n",
    "\n",
    "    # Evaluate whether extra padding is warranted or not\n",
    "    last_wav_range = wav_slices[-1]\n",
    "    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n",
    "    if coverage < min_pad_coverage and len(mel_slices) > 1:\n",
    "        mel_slices = mel_slices[:-1]\n",
    "        wav_slices = wav_slices[:-1]\n",
    "\n",
    "    return wav_slices, mel_slices\n",
    "\n",
    "\n",
    "def Eembed_utterance(wav, using_partials=True, return_partials=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes an embedding for a single utterance.\n",
    "\n",
    "    # TODO: handle multiple wavs to benefit from batching on GPU\n",
    "    :param wav: a preprocessed (see audio.py) utterance waveform as a numpy array of float32\n",
    "    :param using_partials: if True, then the utterance is split in partial utterances of\n",
    "    <partial_utterance_n_frames> frames and the utterance embedding is computed from their\n",
    "    normalized average. If False, the utterance is instead computed from feeding the entire\n",
    "    spectogram to the network.\n",
    "    :param return_partials: if True, the partial embeddings will also be returned along with the\n",
    "    wav slices that correspond to the partial embeddings.\n",
    "    :param kwargs: additional arguments to compute_partial_splits()\n",
    "    :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\n",
    "    <return_partials> is True, the partial utterances as a numpy array of float32 of shape\n",
    "    (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\n",
    "    returned. If <using_partials> is simultaneously set to False, both these values will be None\n",
    "    instead.\n",
    "    \"\"\"\n",
    "    # Process the entire utterance if not using partials\n",
    "    if not using_partials:\n",
    "        frames = audio.wav_to_mel_spectrogram(wav)\n",
    "        embed = embed_frames_batch(frames[None, ...])[0]\n",
    "        if return_partials:\n",
    "            return embed, None, None\n",
    "        return embed\n",
    "\n",
    "    # Compute where to split the utterance into partials and pad if necessary\n",
    "    wave_slices, mel_slices = compute_partial_slices(len(wav), **kwargs)\n",
    "    max_wave_length = wave_slices[-1].stop\n",
    "    if max_wave_length >= len(wav):\n",
    "        wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n",
    "\n",
    "    # Split the utterance into partials\n",
    "    frames = wav_to_mel_spectrogram(wav)\n",
    "    frames_batch = np.array([frames[s] for s in mel_slices])\n",
    "    partial_embeds = embed_frames_batch(frames_batch)\n",
    "\n",
    "    # Compute the utterance embedding from the partial embeddings\n",
    "    raw_embed = np.mean(partial_embeds, axis=0)\n",
    "    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n",
    "\n",
    "    if return_partials:\n",
    "        return embed, partial_embeds, wave_slices\n",
    "    return embed\n",
    "\n",
    "\n",
    "def embed_speaker(wavs, **kwargs):\n",
    "    raise NotImplemented()\n",
    "\n",
    "\n",
    "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if shape is None:\n",
    "        height = int(np.sqrt(len(embed)))\n",
    "        shape = (height, -1)\n",
    "    embed = embed.reshape(shape)\n",
    "\n",
    "    cmap = cm.get_cmap()\n",
    "    mappable = ax.imshow(embed, cmap=cmap)\n",
    "    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n",
    "    sm = cm.ScalarMappable(cmap=cmap)\n",
    "    sm.set_clim(*color_range)\n",
    "\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pprint\n",
    "\n",
    "class HParams(object):\n",
    "    def __init__(self, **kwargs): self.__dict__.update(kwargs)\n",
    "    def __setitem__(self, key, value): setattr(self, key, value)\n",
    "    def __getitem__(self, key): return getattr(self, key)\n",
    "    def __repr__(self): return pprint.pformat(self.__dict__)\n",
    "\n",
    "    def parse(self, string):\n",
    "        # Overrides hparams from a comma-separated string of name=value pairs\n",
    "        if len(string) > 0:\n",
    "            overrides = [s.split(\"=\") for s in string.split(\",\")]\n",
    "            keys, values = zip(*overrides)\n",
    "            keys = list(map(str.strip, keys))\n",
    "            values = list(map(str.strip, values))\n",
    "            for k in keys:\n",
    "                self.__dict__[k] = ast.literal_eval(values[keys.index(k)])\n",
    "        return self\n",
    "\n",
    "hparams = HParams(\n",
    "        ### Signal Processing (used in both synthesizer and vocoder)\n",
    "        sample_rate = 16000,\n",
    "        n_fft = 800,\n",
    "        num_mels = 80,\n",
    "        hop_size = 200,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\n",
    "        win_size = 800,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\n",
    "        fmin = 55,\n",
    "        min_level_db = -100,\n",
    "        ref_level_db = 20,\n",
    "        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\n",
    "        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\n",
    "        preemphasize = True,\n",
    "\n",
    "        ### Tacotron Text-to-Speech (TTS)\n",
    "        tts_embed_dims = 512,                       # Embedding dimension for the graphemes/phoneme inputs\n",
    "        tts_encoder_dims = 256,\n",
    "        tts_decoder_dims = 128,\n",
    "        tts_postnet_dims = 512,\n",
    "        tts_encoder_K = 5,\n",
    "        tts_lstm_dims = 1024,\n",
    "        tts_postnet_K = 5,\n",
    "        tts_num_highways = 4,\n",
    "        tts_dropout = 0.5,\n",
    "        tts_cleaner_names = [\"english_cleaners\"],\n",
    "        tts_stop_threshold = -3.4,                  # Value below which audio generation ends.\n",
    "                                                    # For example, for a range of [-4, 4], this\n",
    "                                                    # will terminate the sequence at the first\n",
    "                                                    # frame that has all values < -3.4\n",
    "\n",
    "        ### Tacotron Training\n",
    "        tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n",
    "                        (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n",
    "                        (2,  2e-4,  80_000,  12),   #\n",
    "                        (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n",
    "                        (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n",
    "                        (2,  1e-5, 640_000,  12)],  # lr = learning rate\n",
    "\n",
    "        tts_clip_grad_norm = 1.0,                   # clips the gradient norm to prevent explosion - set to None if not needed\n",
    "        tts_eval_interval = 500,                    # Number of steps between model evaluation (sample generation)\n",
    "                                                    # Set to -1 to generate after completing epoch, or 0 to disable\n",
    "\n",
    "        tts_eval_num_samples = 1,                   # Makes this number of samples\n",
    "\n",
    "        ### Data Preprocessing\n",
    "        max_mel_frames = 900,\n",
    "        rescale = True,\n",
    "        rescaling_max = 0.9,\n",
    "        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n",
    "\n",
    "        ### Mel Visualization and Griffin-Lim\n",
    "        signal_normalization = True,\n",
    "        power = 1.5,\n",
    "        griffin_lim_iters = 60,\n",
    "\n",
    "        ### Audio processing options\n",
    "        fmax = 7600,                                # Should not exceed (sample_rate // 2)\n",
    "        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\n",
    "        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\n",
    "        use_lws = False,                            # \"Fast spectrogram phase recovery using local weighted sums\"\n",
    "        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\n",
    "                                                    #               and [0, max_abs_value] if False\n",
    "        trim_silence = True,                        # Use with sample_rate of 16000 for best results\n",
    "\n",
    "        ### SV2TTS\n",
    "        speaker_embedding_size = 256,               # Dimension for the speaker embedding\n",
    "        silence_min_duration_split = 0.4,           # Duration in seconds of a silence for an utterance to be split\n",
    "        utterance_min_duration = 1.6,               # Duration in seconds below which utterances are discarded\n",
    "        )\n",
    "\n",
    "def hparams_debug_string():\n",
    "    return str(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.filters\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def load_wav(path, sr):\n",
    "    return librosa.core.load(path, sr=sr)[0]\n",
    "\n",
    "def save_wav(wav, path, sr):\n",
    "    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n",
    "    #proposed by @dsmiller\n",
    "    wavfile.write(path, sr, wav.astype(np.int16))\n",
    "\n",
    "def save_wavenet_wav(wav, path, sr):\n",
    "    sf.write(path, wav.astype(np.float32), sr)\n",
    "\n",
    "def preemphasis(wav, k, preemphasize=True):\n",
    "    if preemphasize:\n",
    "        return signal.lfilter([1, -k], [1], wav)\n",
    "    return wav\n",
    "\n",
    "def inv_preemphasis(wav, k, inv_preemphasize=True):\n",
    "    if inv_preemphasize:\n",
    "        return signal.lfilter([1], [1, -k], wav)\n",
    "    return wav\n",
    "\n",
    "#From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py\n",
    "def start_and_end_indices(quantized, silence_threshold=2):\n",
    "    for start in range(quantized.size):\n",
    "        if abs(quantized[start] - 127) > silence_threshold:\n",
    "            break\n",
    "    for end in range(quantized.size - 1, 1, -1):\n",
    "        if abs(quantized[end] - 127) > silence_threshold:\n",
    "            break\n",
    "    \n",
    "    assert abs(quantized[start] - 127) > silence_threshold\n",
    "    assert abs(quantized[end] - 127) > silence_threshold\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "def get_hop_size(hparams):\n",
    "    hop_size = hparams.hop_size\n",
    "    if hop_size is None:\n",
    "        assert hparams.frame_shift_ms is not None\n",
    "        hop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n",
    "    return hop_size\n",
    "\n",
    "def linearspectrogram(wav, hparams):\n",
    "    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n",
    "    S = _amp_to_db(np.abs(D), hparams) - hparams.ref_level_db\n",
    "    \n",
    "    if hparams.signal_normalization:\n",
    "        return _normalize(S, hparams)\n",
    "    return S\n",
    "\n",
    "def melspectrogram(wav, hparams):\n",
    "    D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n",
    "    S = _amp_to_db(_linear_to_mel(np.abs(D), hparams), hparams) - hparams.ref_level_db\n",
    "    \n",
    "    if hparams.signal_normalization:\n",
    "        return _normalize(S, hparams)\n",
    "    return S\n",
    "\n",
    "def inv_linear_spectrogram(linear_spectrogram, hparams):\n",
    "    \"\"\"Converts linear spectrogram to waveform using librosa\"\"\"\n",
    "    if hparams.signal_normalization:\n",
    "        D = _denormalize(linear_spectrogram, hparams)\n",
    "    else:\n",
    "        D = linear_spectrogram\n",
    "    \n",
    "    S = _db_to_amp(D + hparams.ref_level_db) #Convert back to linear\n",
    "    \n",
    "    if hparams.use_lws:\n",
    "        processor = _lws_processor(hparams)\n",
    "        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n",
    "        y = processor.istft(D).astype(np.float32)\n",
    "        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n",
    "    else:\n",
    "        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n",
    "\n",
    "def inv_mel_spectrogram(mel_spectrogram, hparams):\n",
    "    \"\"\"Converts mel spectrogram to waveform using librosa\"\"\"\n",
    "    if hparams.signal_normalization:\n",
    "        D = _denormalize(mel_spectrogram, hparams)\n",
    "    else:\n",
    "        D = mel_spectrogram\n",
    "    \n",
    "    S = _mel_to_linear(_db_to_amp(D + hparams.ref_level_db), hparams)  # Convert back to linear\n",
    "    \n",
    "    if hparams.use_lws:\n",
    "        processor = _lws_processor(hparams)\n",
    "        D = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n",
    "        y = processor.istft(D).astype(np.float32)\n",
    "        return inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n",
    "    else:\n",
    "        return inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n",
    "\n",
    "def _lws_processor(hparams):\n",
    "    import lws\n",
    "    return lws.lws(hparams.n_fft, get_hop_size(hparams), fftsize=hparams.win_size, mode=\"speech\")\n",
    "\n",
    "def _griffin_lim(S, hparams):\n",
    "    \"\"\"librosa implementation of Griffin-Lim\n",
    "    Based on https://github.com/librosa/librosa/issues/434\n",
    "    \"\"\"\n",
    "    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n",
    "    S_complex = np.abs(S).astype(np.complex)\n",
    "    y = _istft(S_complex * angles, hparams)\n",
    "    for i in range(hparams.griffin_lim_iters):\n",
    "        angles = np.exp(1j * np.angle(_stft(y, hparams)))\n",
    "        y = _istft(S_complex * angles, hparams)\n",
    "    return y\n",
    "\n",
    "def _stft(y, hparams):\n",
    "    if hparams.use_lws:\n",
    "        return _lws_processor(hparams).stft(y).T\n",
    "    else:\n",
    "        return librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n",
    "\n",
    "def _istft(y, hparams):\n",
    "    return librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n",
    "\n",
    "##########################################################\n",
    "#Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\n",
    "def num_frames(length, fsize, fshift):\n",
    "    \"\"\"Compute number of time frames of spectrogram\n",
    "    \"\"\"\n",
    "    pad = (fsize - fshift)\n",
    "    if length % fshift == 0:\n",
    "        M = (length + pad * 2 - fsize) // fshift + 1\n",
    "    else:\n",
    "        M = (length + pad * 2 - fsize) // fshift + 2\n",
    "    return M\n",
    "\n",
    "\n",
    "def pad_lr(x, fsize, fshift):\n",
    "    \"\"\"Compute left and right padding\n",
    "    \"\"\"\n",
    "    M = num_frames(len(x), fsize, fshift)\n",
    "    pad = (fsize - fshift)\n",
    "    T = len(x) + 2 * pad\n",
    "    r = (M - 1) * fshift + fsize - T\n",
    "    return pad, pad + r\n",
    "##########################################################\n",
    "#Librosa correct padding\n",
    "def librosa_pad_lr(x, fsize, fshift):\n",
    "    return 0, (x.shape[0] // fshift + 1) * fshift - x.shape[0]\n",
    "\n",
    "# Conversions\n",
    "_mel_basis = None\n",
    "_inv_mel_basis = None\n",
    "\n",
    "def _linear_to_mel(spectogram, hparams):\n",
    "    global _mel_basis\n",
    "    if _mel_basis is None:\n",
    "        _mel_basis = _build_mel_basis(hparams)\n",
    "    return np.dot(_mel_basis, spectogram)\n",
    "\n",
    "def _mel_to_linear(mel_spectrogram, hparams):\n",
    "    global _inv_mel_basis\n",
    "    if _inv_mel_basis is None:\n",
    "        _inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n",
    "    return np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n",
    "\n",
    "def _build_mel_basis(hparams):\n",
    "    assert hparams.fmax <= hparams.sample_rate // 2\n",
    "    return librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,\n",
    "                               fmin=hparams.fmin, fmax=hparams.fmax)\n",
    "\n",
    "def _amp_to_db(x, hparams):\n",
    "    min_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n",
    "    return 20 * np.log10(np.maximum(min_level, x))\n",
    "\n",
    "def _db_to_amp(x):\n",
    "    return np.power(10.0, (x) * 0.05)\n",
    "\n",
    "def _normalize(S, hparams):\n",
    "    if hparams.allow_clipping_in_normalization:\n",
    "        if hparams.symmetric_mels:\n",
    "            return np.clip((2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value,\n",
    "                           -hparams.max_abs_value, hparams.max_abs_value)\n",
    "        else:\n",
    "            return np.clip(hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db)), 0, hparams.max_abs_value)\n",
    "    \n",
    "    assert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n",
    "    if hparams.symmetric_mels:\n",
    "        return (2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value\n",
    "    else:\n",
    "        return hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db))\n",
    "\n",
    "def _denormalize(D, hparams):\n",
    "    if hparams.allow_clipping_in_normalization:\n",
    "        if hparams.symmetric_mels:\n",
    "            return (((np.clip(D, -hparams.max_abs_value,\n",
    "                              hparams.max_abs_value) + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value))\n",
    "                    + hparams.min_level_db)\n",
    "        else:\n",
    "            return ((np.clip(D, 0, hparams.max_abs_value) * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n",
    "    \n",
    "    if hparams.symmetric_mels:\n",
    "        return (((D + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value)) + hparams.min_level_db)\n",
    "    else:\n",
    "        return ((D * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class HighwayNetwork(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(size, size)\n",
    "        self.W2 = nn.Linear(size, size)\n",
    "        self.W1.bias.data.fill_(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.W1(x)\n",
    "        x2 = self.W2(x)\n",
    "        g = torch.sigmoid(x2)\n",
    "        y = g * F.relu(x1) + (1. - g) * x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dims, num_chars, encoder_dims, K, num_highways, dropout):\n",
    "        super().__init__()\n",
    "        prenet_dims = (encoder_dims, encoder_dims)\n",
    "        cbhg_channels = encoder_dims\n",
    "        self.embedding = nn.Embedding(num_chars, embed_dims)\n",
    "        self.pre_net = PreNet(embed_dims, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                              dropout=dropout)\n",
    "        self.cbhg = CBHG(K=K, in_channels=cbhg_channels, channels=cbhg_channels,\n",
    "                         proj_channels=[cbhg_channels, cbhg_channels],\n",
    "                         num_highways=num_highways)\n",
    "\n",
    "    def forward(self, x, speaker_embedding=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pre_net(x)\n",
    "        x.transpose_(1, 2)\n",
    "        x = self.cbhg(x)\n",
    "        if speaker_embedding is not None:\n",
    "            x = self.add_speaker_embedding(x, speaker_embedding)\n",
    "        return x\n",
    "\n",
    "    def add_speaker_embedding(self, x, speaker_embedding):\n",
    "        # SV2TTS\n",
    "        # The input x is the encoder output and is a 3D tensor with size (batch_size, num_chars, tts_embed_dims)\n",
    "        # When training, speaker_embedding is also a 2D tensor with size (batch_size, speaker_embedding_size)\n",
    "        #     (for inference, speaker_embedding is a 1D tensor with size (speaker_embedding_size))\n",
    "        # This concats the speaker embedding for each char in the encoder output\n",
    "\n",
    "        # Save the dimensions as human-readable names\n",
    "        batch_size = x.size()[0]\n",
    "        num_chars = x.size()[1]\n",
    "\n",
    "        if speaker_embedding.dim() == 1:\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx = 1\n",
    "\n",
    "        # Start by making a copy of each speaker embedding to match the input text length\n",
    "        # The output of this has size (batch_size, num_chars * tts_embed_dims)\n",
    "        speaker_embedding_size = speaker_embedding.size()[idx]\n",
    "        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n",
    "\n",
    "        # Reshape it and transpose\n",
    "        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n",
    "        e = e.transpose(1, 2)\n",
    "\n",
    "        # Concatenate the tiled speaker embedding with the encoder output\n",
    "        x = torch.cat((x, e), 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchNormConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, relu=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n",
    "        self.bnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x) if self.relu is True else x\n",
    "        return self.bnorm(x)\n",
    "\n",
    "\n",
    "class CBHG(nn.Module):\n",
    "    def __init__(self, K, in_channels, channels, proj_channels, num_highways):\n",
    "        super().__init__()\n",
    "\n",
    "        # List of all rnns to call `flatten_parameters()` on\n",
    "        self._to_flatten = []\n",
    "\n",
    "        self.bank_kernels = [i for i in range(1, K + 1)]\n",
    "        self.conv1d_bank = nn.ModuleList()\n",
    "        for k in self.bank_kernels:\n",
    "            conv = BatchNormConv(in_channels, channels, k)\n",
    "            self.conv1d_bank.append(conv)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "\n",
    "        self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)\n",
    "        self.conv_project2 = BatchNormConv(proj_channels[0], proj_channels[1], 3, relu=False)\n",
    "\n",
    "        # Fix the highway input if necessary\n",
    "        if proj_channels[-1] != channels:\n",
    "            self.highway_mismatch = True\n",
    "            self.pre_highway = nn.Linear(proj_channels[-1], channels, bias=False)\n",
    "        else:\n",
    "            self.highway_mismatch = False\n",
    "\n",
    "        self.highways = nn.ModuleList()\n",
    "        for i in range(num_highways):\n",
    "            hn = HighwayNetwork(channels)\n",
    "            self.highways.append(hn)\n",
    "\n",
    "        self.rnn = nn.GRU(channels, channels // 2, batch_first=True, bidirectional=True)\n",
    "        self._to_flatten.append(self.rnn)\n",
    "\n",
    "        # Avoid fragmentation of RNN parameters and associated warning\n",
    "        self._flatten_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Although we `_flatten_parameters()` on init, when using DataParallel\n",
    "        # the model gets replicated, making it no longer guaranteed that the\n",
    "        # weights are contiguous in GPU memory. Hence, we must call it again\n",
    "        self._flatten_parameters()\n",
    "\n",
    "        # Save these for later\n",
    "        residual = x\n",
    "        seq_len = x.size(-1)\n",
    "        conv_bank = []\n",
    "\n",
    "        # Convolution Bank\n",
    "        for conv in self.conv1d_bank:\n",
    "            c = conv(x) # Convolution\n",
    "            conv_bank.append(c[:, :, :seq_len])\n",
    "\n",
    "        # Stack along the channel axis\n",
    "        conv_bank = torch.cat(conv_bank, dim=1)\n",
    "\n",
    "        # dump the last padding to fit residual\n",
    "        x = self.maxpool(conv_bank)[:, :, :seq_len]\n",
    "\n",
    "        # Conv1d projections\n",
    "        x = self.conv_project1(x)\n",
    "        x = self.conv_project2(x)\n",
    "\n",
    "        # Residual Connect\n",
    "        x = x + residual\n",
    "\n",
    "        # Through the highways\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.highway_mismatch is True:\n",
    "            x = self.pre_highway(x)\n",
    "        for h in self.highways: x = h(x)\n",
    "\n",
    "        # And then the RNN\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "    def _flatten_parameters(self):\n",
    "        \"\"\"Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used\n",
    "        to improve efficiency and avoid PyTorch yelling at us.\"\"\"\n",
    "        [m.flatten_parameters() for m in self._to_flatten]\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    def __init__(self, in_dims, fc1_dims=256, fc2_dims=128, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_dims):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(attn_dims, attn_dims, bias=False)\n",
    "        self.v = nn.Linear(attn_dims, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t):\n",
    "\n",
    "        # print(encoder_seq_proj.shape)\n",
    "        # Transform the query vector\n",
    "        query_proj = self.W(query).unsqueeze(1)\n",
    "\n",
    "        # Compute the scores\n",
    "        u = self.v(torch.tanh(encoder_seq_proj + query_proj))\n",
    "        scores = F.softmax(u, dim=1)\n",
    "\n",
    "        return scores.transpose(1, 2)\n",
    "\n",
    "\n",
    "class LSA(nn.Module):\n",
    "    def __init__(self, attn_dim, kernel_size=31, filters=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, filters, padding=(kernel_size - 1) // 2, kernel_size=kernel_size, bias=True)\n",
    "        self.L = nn.Linear(filters, attn_dim, bias=False)\n",
    "        self.W = nn.Linear(attn_dim, attn_dim, bias=True) # Include the attention bias in this term\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.cumulative = None\n",
    "        self.attention = None\n",
    "\n",
    "    def init_attention(self, encoder_seq_proj):\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "        b, t, c = encoder_seq_proj.size()\n",
    "        self.cumulative = torch.zeros(b, t, device=device)\n",
    "        self.attention = torch.zeros(b, t, device=device)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t, chars):\n",
    "\n",
    "        if t == 0: self.init_attention(encoder_seq_proj)\n",
    "\n",
    "        processed_query = self.W(query).unsqueeze(1)\n",
    "\n",
    "        location = self.cumulative.unsqueeze(1)\n",
    "        processed_loc = self.L(self.conv(location).transpose(1, 2))\n",
    "\n",
    "        u = self.v(torch.tanh(processed_query + encoder_seq_proj + processed_loc))\n",
    "        u = u.squeeze(-1)\n",
    "\n",
    "        # Mask zero padding chars\n",
    "        u = u * (chars != 0).float()\n",
    "\n",
    "        # Smooth Attention\n",
    "        # scores = torch.sigmoid(u) / torch.sigmoid(u).sum(dim=1, keepdim=True)\n",
    "        scores = F.softmax(u, dim=1)\n",
    "        self.attention = scores\n",
    "        self.cumulative = self.cumulative + self.attention\n",
    "\n",
    "        return scores.unsqueeze(-1).transpose(1, 2)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Class variable because its value doesn't change between classes\n",
    "    # yet ought to be scoped by class because its a property of a Decoder\n",
    "    max_r = 20\n",
    "    def __init__(self, n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                 dropout, speaker_embedding_size):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"r\", torch.tensor(1, dtype=torch.int))\n",
    "        self.n_mels = n_mels\n",
    "        prenet_dims = (decoder_dims * 2, decoder_dims * 2)\n",
    "        self.prenet = PreNet(n_mels, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                             dropout=dropout)\n",
    "        self.attn_net = LSA(decoder_dims)\n",
    "        self.attn_rnn = nn.GRUCell(encoder_dims + prenet_dims[1] + speaker_embedding_size, decoder_dims)\n",
    "        self.rnn_input = nn.Linear(encoder_dims + decoder_dims + speaker_embedding_size, lstm_dims)\n",
    "        self.res_rnn1 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.res_rnn2 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.mel_proj = nn.Linear(lstm_dims, n_mels * self.max_r, bias=False)\n",
    "        self.stop_proj = nn.Linear(encoder_dims + speaker_embedding_size + lstm_dims, 1)\n",
    "\n",
    "    def zoneout(self, prev, current, p=0.1):\n",
    "        device = next(self.parameters()).device  # Use same device as parameters\n",
    "        mask = torch.zeros(prev.size(), device=device).bernoulli_(p)\n",
    "        return prev * mask + current * (1 - mask)\n",
    "\n",
    "    def forward(self, encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                hidden_states, cell_states, context_vec, t, chars):\n",
    "\n",
    "        # Need this for reshaping mels\n",
    "        batch_size = encoder_seq.size(0)\n",
    "\n",
    "        # Unpack the hidden and cell states\n",
    "        attn_hidden, rnn1_hidden, rnn2_hidden = hidden_states\n",
    "        rnn1_cell, rnn2_cell = cell_states\n",
    "\n",
    "        # PreNet for the Attention RNN\n",
    "        prenet_out = self.prenet(prenet_in)\n",
    "\n",
    "        # Compute the Attention RNN hidden state\n",
    "        attn_rnn_in = torch.cat([context_vec, prenet_out], dim=-1)\n",
    "        attn_hidden = self.attn_rnn(attn_rnn_in.squeeze(1), attn_hidden)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        scores = self.attn_net(encoder_seq_proj, attn_hidden, t, chars)\n",
    "\n",
    "        # Dot product to create the context vector\n",
    "        context_vec = scores @ encoder_seq\n",
    "        context_vec = context_vec.squeeze(1)\n",
    "\n",
    "        # Concat Attention RNN output w. Context Vector & project\n",
    "        x = torch.cat([context_vec, attn_hidden], dim=1)\n",
    "        x = self.rnn_input(x)\n",
    "\n",
    "        # Compute first Residual RNN\n",
    "        rnn1_hidden_next, rnn1_cell = self.res_rnn1(x, (rnn1_hidden, rnn1_cell))\n",
    "        if self.training:\n",
    "            rnn1_hidden = self.zoneout(rnn1_hidden, rnn1_hidden_next)\n",
    "        else:\n",
    "            rnn1_hidden = rnn1_hidden_next\n",
    "        x = x + rnn1_hidden\n",
    "\n",
    "        # Compute second Residual RNN\n",
    "        rnn2_hidden_next, rnn2_cell = self.res_rnn2(x, (rnn2_hidden, rnn2_cell))\n",
    "        if self.training:\n",
    "            rnn2_hidden = self.zoneout(rnn2_hidden, rnn2_hidden_next)\n",
    "        else:\n",
    "            rnn2_hidden = rnn2_hidden_next\n",
    "        x = x + rnn2_hidden\n",
    "\n",
    "        # Project Mels\n",
    "        mels = self.mel_proj(x)\n",
    "        mels = mels.view(batch_size, self.n_mels, self.max_r)[:, :, :self.r]\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # Stop token prediction\n",
    "        s = torch.cat((x, context_vec), dim=1)\n",
    "        s = self.stop_proj(s)\n",
    "        stop_tokens = torch.sigmoid(s)\n",
    "\n",
    "        return mels, scores, hidden_states, cell_states, context_vec, stop_tokens\n",
    "\n",
    "\n",
    "class Tacotron(nn.Module):\n",
    "    def __init__(self, embed_dims, num_chars, encoder_dims, decoder_dims, n_mels, \n",
    "                 fft_bins, postnet_dims, encoder_K, lstm_dims, postnet_K, num_highways,\n",
    "                 dropout, stop_threshold, speaker_embedding_size):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.lstm_dims = lstm_dims\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.decoder_dims = decoder_dims\n",
    "        self.speaker_embedding_size = speaker_embedding_size\n",
    "        self.encoder = Encoder(embed_dims, num_chars, encoder_dims,\n",
    "                               encoder_K, num_highways, dropout)\n",
    "        self.encoder_proj = nn.Linear(encoder_dims + speaker_embedding_size, decoder_dims, bias=False)\n",
    "        self.decoder = Decoder(n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                               dropout, speaker_embedding_size)\n",
    "        self.postnet = CBHG(postnet_K, n_mels, postnet_dims,\n",
    "                            [postnet_dims, fft_bins], num_highways)\n",
    "        self.post_proj = nn.Linear(postnet_dims, fft_bins, bias=False)\n",
    "\n",
    "        self.init_model()\n",
    "        self.num_params()\n",
    "\n",
    "        self.register_buffer(\"step\", torch.zeros(1, dtype=torch.long))\n",
    "        self.register_buffer(\"stop_threshold\", torch.tensor(stop_threshold, dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return self.decoder.r.item()\n",
    "\n",
    "    @r.setter\n",
    "    def r(self, value):\n",
    "        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, m, speaker_embedding):\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "\n",
    "        self.step += 1\n",
    "        batch_size, _, steps  = m.size()\n",
    "\n",
    "        # Initialise all hidden states and pack into tuple\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        # Initialise all lstm cell states and pack into tuple\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # <GO> Frame for start of decoder loop\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        # Need an initial context vector\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "\n",
    "        # SV2TTS: Run the encoder with the speaker embedding\n",
    "        # The projection avoids unnecessary matmuls in the decoder loop\n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        # Need a couple of lists for outputs\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        # Run the decoder loop\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                             hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "\n",
    "        # Concat the mel outputs into sequence\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        # Post-Process for Linear Spectrograms\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        # For easy visualisation\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        # attn_scores = attn_scores.cpu().data.numpy()\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        return mel_outputs, linear, attn_scores, stop_outputs\n",
    "\n",
    "    def generate(self, x, speaker_embedding=None, steps=2000):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "\n",
    "        batch_size, _  = x.size()\n",
    "\n",
    "        # Need to initialise all hidden states and pack into tuple for tidyness\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        # Need to initialise all lstm cell states and pack into tuple for tidyness\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        # Need a <GO> Frame for start of decoder loop\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        # Need an initial context vector\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "\n",
    "        # SV2TTS: Run the encoder with the speaker embedding\n",
    "        # The projection avoids unnecessary matmuls in the decoder loop\n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        # Need a couple of lists for outputs\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        # Run the decoder loop\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                         hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "            # Stop the loop when all stop tokens in batch exceed threshold\n",
    "            if (stop_tokens > 0.5).all() and t > 10: break\n",
    "\n",
    "        # Concat the mel outputs into sequence\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        # Post-Process for Linear Spectrograms\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "\n",
    "\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        # For easy visualisation\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return mel_outputs, linear, attn_scores\n",
    "\n",
    "    def init_model(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_step(self):\n",
    "        return self.step.data.item()\n",
    "\n",
    "    def reset_step(self):\n",
    "        # assignment to parameters or buffers is overloaded, updates internal dict entry\n",
    "        self.step = self.step.data.new_tensor(1)\n",
    "\n",
    "    def log(self, path, msg):\n",
    "        with open(path, \"a\") as f:\n",
    "            print(msg, file=f)\n",
    "\n",
    "    def load(self, path, optimizer=None):\n",
    "        # Use device of model params as location for loaded state\n",
    "        device = next(self.parameters()).device\n",
    "        checkpoint = torch.load(str(path), map_location=device)\n",
    "        self.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "        if \"optimizer_state\" in checkpoint and optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "    def save(self, path, optimizer=None):\n",
    "        if optimizer is not None:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, str(path))\n",
    "        else:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "            }, str(path))\n",
    "\n",
    "\n",
    "    def num_params(self, print_out=True):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "        if print_out:\n",
    "            print(\"Trainable Parameters: %.3fM\" % parameters)\n",
    "        return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines the set of symbols used in text input to the model.\n",
    "\n",
    "The default is a set of ASCII characters that works well for English or text that has been run\n",
    "through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n",
    "\"\"\"\n",
    "# from . import cmudict\n",
    "\n",
    "_pad        = \"_\"\n",
    "_eos        = \"~\"\n",
    "_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\'\\\"(),-.:;? \"\n",
    "\n",
    "# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n",
    "#_arpabet = [\"@' + s for s in cmudict.valid_symbols]\n",
    "\n",
    "# Export all symbols:\n",
    "symbols = [_pad, _eos] + list(_characters) #+ _arpabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "\n",
    "\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n",
    "_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n",
    "_pounds_re = re.compile(r\"([0-9\\,]*[0-9]+)\")\n",
    "_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n",
    "_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n",
    "_number_re = re.compile(r\"[0-9]+\")\n",
    "\n",
    "\n",
    "def _remove_commas(m):\n",
    "    return m.group(1).replace(\",\", \"\")\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "    return m.group(1).replace(\".\", \" point \")\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "    match = m.group(1)\n",
    "    parts = match.split(\".\")\n",
    "    if len(parts) > 2:\n",
    "        return match + \" dollars\"  # Unexpected format\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "    if dollars and cents:\n",
    "        dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "        cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "        return \"%s %s, %s %s\" % (dollars, dollar_unit, cents, cent_unit)\n",
    "    elif dollars:\n",
    "        dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "        return \"%s %s\" % (dollars, dollar_unit)\n",
    "    elif cents:\n",
    "        cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "        return \"%s %s\" % (cents, cent_unit)\n",
    "    else:\n",
    "        return \"zero dollars\"\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "    return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return \"two thousand\"\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return \"two thousand \" + _inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return _inflect.number_to_words(num // 100) + \" hundred\"\n",
    "        else:\n",
    "            return _inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n",
    "    else:\n",
    "        return _inflect.number_to_words(num, andword=\"\")\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "    text = re.sub(_pounds_re, r\"\\1 pounds\", text)\n",
    "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "    text = re.sub(_number_re, _expand_number, text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaners are transformations that run over the input text at both training and eval time.\n",
    "\n",
    "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
    "hyperparameter. Some cleaners are English-specific. You\"ll typically want to use:\n",
    "  1. \"english_cleaners\" for English text\n",
    "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
    "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
    "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
    "     the symbols in symbols.py to match your data).\n",
    "\"\"\"\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "# from synthesizer.utils.numbers import normalize_numbers\n",
    "\n",
    "\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "    (\"mrs\", \"misess\"),\n",
    "    (\"mr\", \"mister\"),\n",
    "    (\"dr\", \"doctor\"),\n",
    "    (\"st\", \"saint\"),\n",
    "    (\"co\", \"company\"),\n",
    "    (\"jr\", \"junior\"),\n",
    "    (\"maj\", \"major\"),\n",
    "    (\"gen\", \"general\"),\n",
    "    (\"drs\", \"doctors\"),\n",
    "    (\"rev\", \"reverend\"),\n",
    "    (\"lt\", \"lieutenant\"),\n",
    "    (\"hon\", \"honorable\"),\n",
    "    (\"sgt\", \"sergeant\"),\n",
    "    (\"capt\", \"captain\"),\n",
    "    (\"esq\", \"esquire\"),\n",
    "    (\"ltd\", \"limited\"),\n",
    "    (\"col\", \"colonel\"),\n",
    "    (\"ft\", \"fort\"),\n",
    "]]\n",
    "\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    return normalize_numbers(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    \"\"\"lowercase input tokens.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "    return re.sub(_whitespace_re, \" \", text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "\n",
    "def basic_cleaners(text):\n",
    "    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transliteration_cleaners(text):\n",
    "    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def english_cleaners(text):\n",
    "    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthesizer.utils.symbols import symbols\n",
    "# from synthesizer.utils import cleaners\n",
    "import re\n",
    "\n",
    "\n",
    "# Mappings from symbol to numeric ID and vice versa:\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# Regular expression matching text enclosed in curly braces:\n",
    "_curly_re = re.compile(r\"(.*?)\\{(.+?)\\}(.*)\")\n",
    "\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "    \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "\n",
    "      The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
    "      in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "\n",
    "      Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "\n",
    "      Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "\n",
    "    # Check for curly braces and treat their contents as ARPAbet:\n",
    "    while len(text):\n",
    "        m = _curly_re.match(text)\n",
    "        if not m:\n",
    "            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
    "            break\n",
    "        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n",
    "        sequence += _arpabet_to_sequence(m.group(2))\n",
    "        text = m.group(3)\n",
    "\n",
    "    # Append EOS token\n",
    "    sequence.append(_symbol_to_id[\"~\"])\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    \"\"\"Converts a sequence of IDs back to a string\"\"\"\n",
    "    result = \"\"\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            # Enclose ARPAbet back in curly braces:\n",
    "            if len(s) > 1 and s[0] == \"@\":\n",
    "                s = \"{%s}\" % s[1:]\n",
    "            result += s\n",
    "    return result.replace(\"}{\", \" \")\n",
    "\n",
    "\n",
    "def _clean_text(text, cleaner_names):\n",
    "    for name in cleaner_names:\n",
    "        cleaner = getattr(cleaners, name)\n",
    "        if not cleaner:\n",
    "            raise Exception(\"Unknown cleaner: %s\" % name)\n",
    "        text = cleaner(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _symbols_to_sequence(symbols):\n",
    "    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n",
    "\n",
    "\n",
    "def _arpabet_to_sequence(text):\n",
    "    return _symbols_to_sequence([\"@\" + s for s in text.split()])\n",
    "\n",
    "\n",
    "def _should_keep_symbol(s):\n",
    "    return s in _symbol_to_id and s not in (\"_\", \"~\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def progbar(i, n, size=16):\n",
    "    done = (i * size) // n\n",
    "    bar = ''\n",
    "    for i in range(size):\n",
    "        bar += '' if i <= done else ''\n",
    "    return bar\n",
    "\n",
    "\n",
    "def stream(message) :\n",
    "    try:\n",
    "        sys.stdout.write(\"\\r{%s}\" % message)\n",
    "    except:\n",
    "        #Remove non-ASCII characters from message\n",
    "        message = ''.join(i for i in message if ord(i)<128)\n",
    "        sys.stdout.write(\"\\r{%s}\" % message)\n",
    "\n",
    "\n",
    "def simple_table(item_tuples) :\n",
    "\n",
    "    border_pattern = '+---------------------------------------'\n",
    "    whitespace = '                                            '\n",
    "\n",
    "    headings, cells, = [], []\n",
    "\n",
    "    for item in item_tuples :\n",
    "\n",
    "        heading, cell = str(item[0]), str(item[1])\n",
    "\n",
    "        pad_head = True if len(heading) < len(cell) else False\n",
    "\n",
    "        pad = abs(len(heading) - len(cell))\n",
    "        pad = whitespace[:pad]\n",
    "\n",
    "        pad_left = pad[:len(pad)//2]\n",
    "        pad_right = pad[len(pad)//2:]\n",
    "\n",
    "        if pad_head :\n",
    "            heading = pad_left + heading + pad_right\n",
    "        else :\n",
    "            cell = pad_left + cell + pad_right\n",
    "\n",
    "        headings += [heading]\n",
    "        cells += [cell]\n",
    "\n",
    "    border, head, body = '', '', ''\n",
    "\n",
    "    for i in range(len(item_tuples)) :\n",
    "\n",
    "        temp_head = f'| {headings[i]} '\n",
    "        temp_body = f'| {cells[i]} '\n",
    "\n",
    "        border += border_pattern[:len(temp_head)]\n",
    "        head += temp_head\n",
    "        body += temp_body\n",
    "\n",
    "        if i == len(item_tuples) - 1 :\n",
    "            head += '|'\n",
    "            body += '|'\n",
    "            border += '+'\n",
    "\n",
    "    print(border)\n",
    "    print(head)\n",
    "    print(border)\n",
    "    print(body)\n",
    "    print(border)\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "def time_since(started) :\n",
    "    elapsed = time.time() - started\n",
    "    m = int(elapsed // 60)\n",
    "    s = int(elapsed % 60)\n",
    "    if m >= 60 :\n",
    "        h = int(m // 60)\n",
    "        m = m % 60\n",
    "        return f'{h}h {m}m {s}s'\n",
    "    else :\n",
    "        return f'{m}m {s}s'\n",
    "\n",
    "\n",
    "def save_attention(attn, path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n",
    "    fig.savefig(f'{path}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_spectrogram(M, path, length=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    M = np.flip(M, axis=0)\n",
    "    if length : M = M[:, :length]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(M, interpolation='nearest', aspect='auto')\n",
    "    fig.savefig(f'{path}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot(array):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.xaxis.label.set_color('grey')\n",
    "    ax.yaxis.label.set_color('grey')\n",
    "    ax.xaxis.label.set_fontsize(23)\n",
    "    ax.yaxis.label.set_fontsize(23)\n",
    "    ax.tick_params(axis='x', colors='grey', labelsize=23)\n",
    "    ax.tick_params(axis='y', colors='grey', labelsize=23)\n",
    "    plt.plot(array)\n",
    "\n",
    "\n",
    "def plot_spec(M):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    M = np.flip(M, axis=0)\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.imshow(M, interpolation='nearest', aspect='auto')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesizer\n",
    "\n",
    "import torch\n",
    "# from synthesizer import audio\n",
    "# from synthesizer.hparams import hparams\n",
    "# from synthesizer.models.tacotron import Tacotron\n",
    "# from synthesizer.utils.symbols import symbols\n",
    "# from synthesizer.utils.text import text_to_sequence\n",
    "# from vocoder.display import simple_table\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "\n",
    "class Synthesizer:\n",
    "    sample_rate = hparams.sample_rate\n",
    "    hparams = hparams\n",
    "\n",
    "    def __init__(self, model_fpath: Path, verbose=True):\n",
    "        \"\"\"\n",
    "        The model isn't instantiated and loaded in memory until needed or until load() is called.\n",
    "\n",
    "        :param model_fpath: path to the trained model file\n",
    "        :param verbose: if False, prints less information when using the model\n",
    "        \"\"\"\n",
    "        self.model_fpath = model_fpath\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Check for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        if self.verbose:\n",
    "            print(\"Synthesizer using device:\", self.device)\n",
    "\n",
    "        # Tacotron model will be instantiated later on first use.\n",
    "        self._model = None\n",
    "\n",
    "    def is_loaded(self):\n",
    "        \"\"\"\n",
    "        Whether the model is loaded in memory.\n",
    "        \"\"\"\n",
    "        return self._model is not None\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Instantiates and loads the model given the weights file that was passed in the constructor.\n",
    "        \"\"\"\n",
    "        self._model = Tacotron(embed_dims=hparams.tts_embed_dims,\n",
    "                               num_chars=len(symbols),\n",
    "                               encoder_dims=hparams.tts_encoder_dims,\n",
    "                               decoder_dims=hparams.tts_decoder_dims,\n",
    "                               n_mels=hparams.num_mels,\n",
    "                               fft_bins=hparams.num_mels,\n",
    "                               postnet_dims=hparams.tts_postnet_dims,\n",
    "                               encoder_K=hparams.tts_encoder_K,\n",
    "                               lstm_dims=hparams.tts_lstm_dims,\n",
    "                               postnet_K=hparams.tts_postnet_K,\n",
    "                               num_highways=hparams.tts_num_highways,\n",
    "                               dropout=hparams.tts_dropout,\n",
    "                               stop_threshold=hparams.tts_stop_threshold,\n",
    "                               speaker_embedding_size=hparams.speaker_embedding_size).to(self.device)\n",
    "\n",
    "        self._model.load(self.model_fpath)\n",
    "        self._model.eval()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded synthesizer \\\"%s\\\" trained to step %d\" % (self.model_fpath.name, self._model.state_dict()[\"step\"]))\n",
    "\n",
    "    def synthesize_spectrograms(self, texts: List[str],\n",
    "                                embeddings: Union[np.ndarray, List[np.ndarray]],\n",
    "                                return_alignments=False):\n",
    "        \"\"\"\n",
    "        Synthesizes mel spectrograms from texts and speaker embeddings.\n",
    "\n",
    "        :param texts: a list of N text prompts to be synthesized\n",
    "        :param embeddings: a numpy array or list of speaker embeddings of shape (N, 256)\n",
    "        :param return_alignments: if True, a matrix representing the alignments between the\n",
    "        characters\n",
    "        and each decoder output step will be returned for each spectrogram\n",
    "        :return: a list of N melspectrograms as numpy arrays of shape (80, Mi), where Mi is the\n",
    "        sequence length of spectrogram i, and possibly the alignments.\n",
    "        \"\"\"\n",
    "        # Load the model on the first request.\n",
    "        if not self.is_loaded():\n",
    "            self.load()\n",
    "\n",
    "        # Preprocess text inputs\n",
    "        inputs = [text_to_sequence(text.strip(), hparams.tts_cleaner_names) for text in texts]\n",
    "        if not isinstance(embeddings, list):\n",
    "            embeddings = [embeddings]\n",
    "\n",
    "        # Batch inputs\n",
    "        batched_inputs = [inputs[i:i+hparams.synthesis_batch_size]\n",
    "                             for i in range(0, len(inputs), hparams.synthesis_batch_size)]\n",
    "        batched_embeds = [embeddings[i:i+hparams.synthesis_batch_size]\n",
    "                             for i in range(0, len(embeddings), hparams.synthesis_batch_size)]\n",
    "\n",
    "        specs = []\n",
    "        for i, batch in enumerate(batched_inputs, 1):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n| Generating {i}/{len(batched_inputs)}\")\n",
    "\n",
    "            # Pad texts so they are all the same length\n",
    "            text_lens = [len(text) for text in batch]\n",
    "            max_text_len = max(text_lens)\n",
    "            chars = [pad1d(text, max_text_len) for text in batch]\n",
    "            chars = np.stack(chars)\n",
    "\n",
    "            # Stack speaker embeddings into 2D array for batch processing\n",
    "            speaker_embeds = np.stack(batched_embeds[i-1])\n",
    "\n",
    "            # Convert to tensor\n",
    "            chars = torch.tensor(chars).long().to(self.device)\n",
    "            speaker_embeddings = torch.tensor(speaker_embeds).float().to(self.device)\n",
    "\n",
    "            # Inference\n",
    "            _, mels, alignments = self._model.generate(chars, speaker_embeddings)\n",
    "            mels = mels.detach().cpu().numpy()\n",
    "            for m in mels:\n",
    "                # Trim silence from end of each spectrogram\n",
    "                while np.max(m[:, -1]) < hparams.tts_stop_threshold:\n",
    "                    m = m[:, :-1]\n",
    "                specs.append(m)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\\nDone.\\n\")\n",
    "        return (specs, alignments) if return_alignments else specs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_preprocess_wav(fpath):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses an audio file under the same conditions the audio files were used to\n",
    "        train the synthesizer.\n",
    "        \"\"\"\n",
    "        wav = librosa.load(str(fpath), hparams.sample_rate)[0]\n",
    "        if hparams.rescale:\n",
    "            wav = wav / np.abs(wav).max() * hparams.rescaling_max\n",
    "        return wav\n",
    "\n",
    "    @staticmethod\n",
    "    def make_spectrogram(fpath_or_wav: Union[str, Path, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Creates a mel spectrogram from an audio file in the same manner as the mel spectrograms that\n",
    "        were fed to the synthesizer when training.\n",
    "        \"\"\"\n",
    "        if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n",
    "            wav = Synthesizer.load_preprocess_wav(fpath_or_wav)\n",
    "        else:\n",
    "            wav = fpath_or_wav\n",
    "\n",
    "        mel_spectrogram = melspectrogram(wav, hparams).astype(np.float32)\n",
    "        return mel_spectrogram\n",
    "\n",
    "    @staticmethod\n",
    "    def griffin_lim(mel):\n",
    "        \"\"\"\n",
    "        Inverts a mel spectrogram using Griffin-Lim. The mel spectrogram is expected to have been built\n",
    "        with the same parameters present in hparams.py.\n",
    "        \"\"\"\n",
    "        return inv_mel_spectrogram(mel, hparams)\n",
    "\n",
    "\n",
    "def pad1d(x, max_len, pad_value=0):\n",
    "    return np.pad(x, (0, max_len - len(x)), mode=\"constant\", constant_values=pad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\" numerically stable log_sum_exp implementation that prevents overflow \"\"\"\n",
    "    # TF ordering\n",
    "    axis = len(x.size()) - 1\n",
    "    m, _ = torch.max(x, dim=axis)\n",
    "    m2, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n",
    "\n",
    "\n",
    "# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\n",
    "def discretized_mix_logistic_loss(y_hat, y, num_classes=65536,\n",
    "                                  log_scale_min=None, reduce=True):\n",
    "    if log_scale_min is None:\n",
    "        log_scale_min = float(np.log(1e-14))\n",
    "    y_hat = y_hat.permute(0,2,1)\n",
    "    assert y_hat.dim() == 3\n",
    "    assert y_hat.size(1) % 3 == 0\n",
    "    nr_mix = y_hat.size(1) // 3\n",
    "\n",
    "    # (B x T x C)\n",
    "    y_hat = y_hat.transpose(1, 2)\n",
    "\n",
    "    # unpack parameters. (B, T, num_mixtures) x 3\n",
    "    logit_probs = y_hat[:, :, :nr_mix]\n",
    "    means = y_hat[:, :, nr_mix:2 * nr_mix]\n",
    "    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n",
    "\n",
    "    # B x T x 1 -> B x T x num_mixtures\n",
    "    y = y.expand_as(means)\n",
    "\n",
    "    centered_y = y - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n",
    "    cdf_plus = torch.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n",
    "    cdf_min = torch.sigmoid(min_in)\n",
    "\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    # equivalent: torch.log(F.sigmoid(plus_in))\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    # equivalent: (1 - F.sigmoid(min_in)).log()\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "\n",
    "    # probability for all other cases\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "\n",
    "    mid_in = inv_stdv * centered_y\n",
    "    # log probability in the center of the bin, to be used in extreme cases\n",
    "    # (not actually used in our code)\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "\n",
    "    # tf equivalent\n",
    "    \"\"\"\n",
    "    log_probs = tf.where(x < -0.999, log_cdf_plus,\n",
    "                         tf.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                  tf.where(cdf_delta > 1e-5,\n",
    "                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n",
    "                                           log_pdf_mid - np.log(127.5))))\n",
    "    \"\"\"\n",
    "    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n",
    "    # for num_classes=65536 case? 1e-7? not sure..\n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "\n",
    "    inner_inner_out = inner_inner_cond * \\\n",
    "        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n",
    "        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n",
    "    inner_cond = (y > 0.999).float()\n",
    "    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond = (y < -0.999).float()\n",
    "    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "\n",
    "    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n",
    "\n",
    "    if reduce:\n",
    "        return -torch.mean(log_sum_exp(log_probs))\n",
    "    else:\n",
    "        return -log_sum_exp(log_probs).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def sample_from_discretized_mix_logistic(y, log_scale_min=None):\n",
    "    \"\"\"\n",
    "    Sample from discretized mixture of logistic distributions\n",
    "    Args:\n",
    "        y (Tensor): B x C x T\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "    Returns:\n",
    "        Tensor: sample in range of [-1, 1].\n",
    "    \"\"\"\n",
    "    if log_scale_min is None:\n",
    "        log_scale_min = float(np.log(1e-14))\n",
    "    assert y.size(1) % 3 == 0\n",
    "    nr_mix = y.size(1) // 3\n",
    "\n",
    "    # B x T x C\n",
    "    y = y.transpose(1, 2)\n",
    "    logit_probs = y[:, :, :nr_mix]\n",
    "\n",
    "    # sample mixture indicator from softmax\n",
    "    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n",
    "    temp = logit_probs.data - torch.log(- torch.log(temp))\n",
    "    _, argmax = temp.max(dim=-1)\n",
    "\n",
    "    # (B, T) -> (B, T, nr_mix)\n",
    "    one_hot = to_one_hot(argmax, nr_mix)\n",
    "    # select logistic parameters\n",
    "    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n",
    "    log_scales = torch.clamp(torch.sum(\n",
    "        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n",
    "    # sample from logistic & clip to interval\n",
    "    # we don't actually round to the nearest 8bit value when sampling\n",
    "    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n",
    "    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n",
    "\n",
    "    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def to_one_hot(tensor, n, fill_with=1.):\n",
    "    # we perform one hot encore with respect to the last axis\n",
    "    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n",
    "    if tensor.is_cuda:\n",
    "        one_hot = one_hot.cuda()\n",
    "    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def progbar(i, n, size=16):\n",
    "    done = (i * size) // n\n",
    "    bar = ''\n",
    "    for i in range(size):\n",
    "        bar += '' if i <= done else ''\n",
    "    return bar\n",
    "\n",
    "\n",
    "def stream(message) :\n",
    "    try:\n",
    "        sys.stdout.write(\"\\r{%s}\" % message)\n",
    "    except:\n",
    "        #Remove non-ASCII characters from message\n",
    "        message = ''.join(i for i in message if ord(i)<128)\n",
    "        sys.stdout.write(\"\\r{%s}\" % message)\n",
    "\n",
    "\n",
    "def simple_table(item_tuples) :\n",
    "\n",
    "    border_pattern = '+---------------------------------------'\n",
    "    whitespace = '                                            '\n",
    "\n",
    "    headings, cells, = [], []\n",
    "\n",
    "    for item in item_tuples :\n",
    "\n",
    "        heading, cell = str(item[0]), str(item[1])\n",
    "\n",
    "        pad_head = True if len(heading) < len(cell) else False\n",
    "\n",
    "        pad = abs(len(heading) - len(cell))\n",
    "        pad = whitespace[:pad]\n",
    "\n",
    "        pad_left = pad[:len(pad)//2]\n",
    "        pad_right = pad[len(pad)//2:]\n",
    "\n",
    "        if pad_head :\n",
    "            heading = pad_left + heading + pad_right\n",
    "        else :\n",
    "            cell = pad_left + cell + pad_right\n",
    "\n",
    "        headings += [heading]\n",
    "        cells += [cell]\n",
    "\n",
    "    border, head, body = '', '', ''\n",
    "\n",
    "    for i in range(len(item_tuples)) :\n",
    "\n",
    "        temp_head = f'| {headings[i]} '\n",
    "        temp_body = f'| {cells[i]} '\n",
    "\n",
    "        border += border_pattern[:len(temp_head)]\n",
    "        head += temp_head\n",
    "        body += temp_body\n",
    "\n",
    "        if i == len(item_tuples) - 1 :\n",
    "            head += '|'\n",
    "            body += '|'\n",
    "            border += '+'\n",
    "\n",
    "    print(border)\n",
    "    print(head)\n",
    "    print(border)\n",
    "    print(body)\n",
    "    print(border)\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "def time_since(started) :\n",
    "    elapsed = time.time() - started\n",
    "    m = int(elapsed // 60)\n",
    "    s = int(elapsed % 60)\n",
    "    if m >= 60 :\n",
    "        h = int(m // 60)\n",
    "        m = m % 60\n",
    "        return f'{h}h {m}m {s}s'\n",
    "    else :\n",
    "        return f'{m}m {s}s'\n",
    "\n",
    "\n",
    "def save_attention(attn, path):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n",
    "    fig.savefig(f'{path}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_spectrogram(M, path, length=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    M = np.flip(M, axis=0)\n",
    "    if length : M = M[:, :length]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(M, interpolation='nearest', aspect='auto')\n",
    "    fig.savefig(f'{path}.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot(array):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.xaxis.label.set_color('grey')\n",
    "    ax.yaxis.label.set_color('grey')\n",
    "    ax.xaxis.label.set_fontsize(23)\n",
    "    ax.yaxis.label.set_fontsize(23)\n",
    "    ax.tick_params(axis='x', colors='grey', labelsize=23)\n",
    "    ax.tick_params(axis='y', colors='grey', labelsize=23)\n",
    "    plt.plot(array)\n",
    "\n",
    "\n",
    "def plot_spec(M):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    M = np.flip(M, axis=0)\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.imshow(M, interpolation='nearest', aspect='auto')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from vocoder.distribution import sample_from_discretized_mix_logistic\n",
    "# from vocoder.display import *\n",
    "# from vocoder.audio import *\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(dims)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class MelResNet(nn.Module):\n",
    "    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):\n",
    "        super().__init__()\n",
    "        k_size = pad * 2 + 1\n",
    "        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm1d(compute_dims)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(res_blocks):\n",
    "            self.layers.append(ResBlock(compute_dims))\n",
    "        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(x)\n",
    "        for f in self.layers: x = f(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stretch2d(nn.Module):\n",
    "    def __init__(self, x_scale, y_scale):\n",
    "        super().__init__()\n",
    "        self.x_scale = x_scale\n",
    "        self.y_scale = y_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.unsqueeze(-1).unsqueeze(3)\n",
    "        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n",
    "        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n",
    "\n",
    "\n",
    "class UpsampleNetwork(nn.Module):\n",
    "    def __init__(self, feat_dims, upsample_scales, compute_dims,\n",
    "                 res_blocks, res_out_dims, pad):\n",
    "        super().__init__()\n",
    "        total_scale = np.cumproduct(upsample_scales)[-1]\n",
    "        self.indent = pad * total_scale\n",
    "        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n",
    "        self.resnet_stretch = Stretch2d(total_scale, 1)\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        for scale in upsample_scales:\n",
    "            k_size = (1, scale * 2 + 1)\n",
    "            padding = (0, scale)\n",
    "            stretch = Stretch2d(scale, 1)\n",
    "            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n",
    "            conv.weight.data.fill_(1. / k_size[1])\n",
    "            self.up_layers.append(stretch)\n",
    "            self.up_layers.append(conv)\n",
    "\n",
    "    def forward(self, m):\n",
    "        aux = self.resnet(m).unsqueeze(1)\n",
    "        aux = self.resnet_stretch(aux)\n",
    "        aux = aux.squeeze(1)\n",
    "        m = m.unsqueeze(1)\n",
    "        for f in self.up_layers: m = f(m)\n",
    "        m = m.squeeze(1)[:, :, self.indent:-self.indent]\n",
    "        return m.transpose(1, 2), aux.transpose(1, 2)\n",
    "\n",
    "\n",
    "class WaveRNN(nn.Module):\n",
    "    def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,\n",
    "                 feat_dims, compute_dims, res_out_dims, res_blocks,\n",
    "                 hop_length, sample_rate, mode='RAW'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.pad = pad\n",
    "        if self.mode == 'RAW' :\n",
    "            self.n_classes = 2 ** bits\n",
    "        elif self.mode == 'MOL' :\n",
    "            self.n_classes = 30\n",
    "        else :\n",
    "            RuntimeError(\"Unknown model mode value - \", self.mode)\n",
    "\n",
    "        self.rnn_dims = rnn_dims\n",
    "        self.aux_dims = res_out_dims // 4\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.upsample = UpsampleNetwork(feat_dims, upsample_factors, compute_dims, res_blocks, res_out_dims, pad)\n",
    "        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n",
    "        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n",
    "        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n",
    "        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n",
    "        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n",
    "\n",
    "        self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)\n",
    "        self.num_params()\n",
    "\n",
    "    def forward(self, x, mels):\n",
    "        self.step += 1\n",
    "        bsize = x.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            h1 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n",
    "            h2 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n",
    "        else:\n",
    "            h1 = torch.zeros(1, bsize, self.rnn_dims).cpu()\n",
    "            h2 = torch.zeros(1, bsize, self.rnn_dims).cpu()\n",
    "        mels, aux = self.upsample(mels)\n",
    "\n",
    "        aux_idx = [self.aux_dims * i for i in range(5)]\n",
    "        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n",
    "        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n",
    "        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n",
    "        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n",
    "\n",
    "        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n",
    "        x = self.I(x)\n",
    "        res = x\n",
    "        x, _ = self.rnn1(x, h1)\n",
    "\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = torch.cat([x, a2], dim=2)\n",
    "        x, _ = self.rnn2(x, h2)\n",
    "\n",
    "        x = x + res\n",
    "        x = torch.cat([x, a3], dim=2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = torch.cat([x, a4], dim=2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def generate(self, mels, batched, target, overlap, mu_law, progress_callback=None):\n",
    "        mu_law = mu_law if self.mode == 'RAW' else False\n",
    "        progress_callback = progress_callback or self.gen_display\n",
    "\n",
    "        self.eval()\n",
    "        output = []\n",
    "        start = time.time()\n",
    "        rnn1 = self.get_gru_cell(self.rnn1)\n",
    "        rnn2 = self.get_gru_cell(self.rnn2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if torch.cuda.is_available():\n",
    "                mels = mels.cuda()\n",
    "            else:\n",
    "                mels = mels.cpu()\n",
    "            wave_len = (mels.size(-1) - 1) * self.hop_length\n",
    "            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n",
    "            mels, aux = self.upsample(mels.transpose(1, 2))\n",
    "\n",
    "            if batched:\n",
    "                mels = self.fold_with_overlap(mels, target, overlap)\n",
    "                aux = self.fold_with_overlap(aux, target, overlap)\n",
    "\n",
    "            b_size, seq_len, _ = mels.size()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n",
    "                h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n",
    "                x = torch.zeros(b_size, 1).cuda()\n",
    "            else:\n",
    "                h1 = torch.zeros(b_size, self.rnn_dims).cpu()\n",
    "                h2 = torch.zeros(b_size, self.rnn_dims).cpu()\n",
    "                x = torch.zeros(b_size, 1).cpu()\n",
    "\n",
    "            d = self.aux_dims\n",
    "            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n",
    "\n",
    "            for i in range(seq_len):\n",
    "\n",
    "                m_t = mels[:, i, :]\n",
    "\n",
    "                a1_t, a2_t, a3_t, a4_t = (a[:, i, :] for a in aux_split)\n",
    "\n",
    "                x = torch.cat([x, m_t, a1_t], dim=1)\n",
    "                x = self.I(x)\n",
    "                h1 = rnn1(x, h1)\n",
    "\n",
    "                x = x + h1\n",
    "                inp = torch.cat([x, a2_t], dim=1)\n",
    "                h2 = rnn2(inp, h2)\n",
    "\n",
    "                x = x + h2\n",
    "                x = torch.cat([x, a3_t], dim=1)\n",
    "                x = F.relu(self.fc1(x))\n",
    "\n",
    "                x = torch.cat([x, a4_t], dim=1)\n",
    "                x = F.relu(self.fc2(x))\n",
    "\n",
    "                logits = self.fc3(x)\n",
    "\n",
    "                if self.mode == 'MOL':\n",
    "                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n",
    "                    output.append(sample.view(-1))\n",
    "                    if torch.cuda.is_available():\n",
    "                        # x = torch.FloatTensor([[sample]]).cuda()\n",
    "                        x = sample.transpose(0, 1).cuda()\n",
    "                    else:\n",
    "                        x = sample.transpose(0, 1)\n",
    "\n",
    "                elif self.mode == 'RAW' :\n",
    "                    posterior = F.softmax(logits, dim=1)\n",
    "                    distrib = torch.distributions.Categorical(posterior)\n",
    "\n",
    "                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n",
    "                    output.append(sample)\n",
    "                    x = sample.unsqueeze(-1)\n",
    "                else:\n",
    "                    raise RuntimeError(\"Unknown model mode value - \", self.mode)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n",
    "                    progress_callback(i, seq_len, b_size, gen_rate)\n",
    "\n",
    "        output = torch.stack(output).transpose(0, 1)\n",
    "        output = output.cpu().numpy()\n",
    "        output = output.astype(np.float64)\n",
    "        \n",
    "        if batched:\n",
    "            output = self.xfade_and_unfold(output, target, overlap)\n",
    "        else:\n",
    "            output = output[0]\n",
    "\n",
    "        if mu_law:\n",
    "            output = decode_mu_law(output, self.n_classes, False)\n",
    "        if hparams.apply_preemphasis:\n",
    "            output = de_emphasis(output)\n",
    "\n",
    "        # Fade-out at the end to avoid signal cutting out suddenly\n",
    "        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n",
    "        output = output[:wave_len]\n",
    "        output[-20 * self.hop_length:] *= fade_out\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def gen_display(self, i, seq_len, b_size, gen_rate):\n",
    "        pbar = progbar(i, seq_len)\n",
    "        msg = f'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | '\n",
    "        stream(msg)\n",
    "\n",
    "    def get_gru_cell(self, gru):\n",
    "        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n",
    "        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n",
    "        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n",
    "        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n",
    "        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n",
    "        return gru_cell\n",
    "\n",
    "    def pad_tensor(self, x, pad, side='both'):\n",
    "        # NB - this is just a quick method i need right now\n",
    "        # i.e., it won't generalise to other shapes/dims\n",
    "        b, t, c = x.size()\n",
    "        total = t + 2 * pad if side == 'both' else t + pad\n",
    "        if torch.cuda.is_available():\n",
    "            padded = torch.zeros(b, total, c).cuda()\n",
    "        else:\n",
    "            padded = torch.zeros(b, total, c).cpu()\n",
    "        if side == 'before' or side == 'both':\n",
    "            padded[:, pad:pad + t, :] = x\n",
    "        elif side == 'after':\n",
    "            padded[:, :t, :] = x\n",
    "        return padded\n",
    "\n",
    "    def fold_with_overlap(self, x, target, overlap):\n",
    "\n",
    "        ''' Fold the tensor with overlap for quick batched inference.\n",
    "            Overlap will be used for crossfading in xfade_and_unfold()\n",
    "\n",
    "        Args:\n",
    "            x (tensor)    : Upsampled conditioning features.\n",
    "                            shape=(1, timesteps, features)\n",
    "            target (int)  : Target timesteps for each index of batch\n",
    "            overlap (int) : Timesteps for both xfade and rnn warmup\n",
    "\n",
    "        Return:\n",
    "            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n",
    "\n",
    "        Details:\n",
    "            x = [[h1, h2, ... hn]]\n",
    "\n",
    "            Where each h is a vector of conditioning features\n",
    "\n",
    "            Eg: target=2, overlap=1 with x.size(1)=10\n",
    "\n",
    "            folded = [[h1, h2, h3, h4],\n",
    "                      [h4, h5, h6, h7],\n",
    "                      [h7, h8, h9, h10]]\n",
    "        '''\n",
    "\n",
    "        _, total_len, features = x.size()\n",
    "\n",
    "        # Calculate variables needed\n",
    "        num_folds = (total_len - overlap) // (target + overlap)\n",
    "        extended_len = num_folds * (overlap + target) + overlap\n",
    "        remaining = total_len - extended_len\n",
    "\n",
    "        # Pad if some time steps poking out\n",
    "        if remaining != 0:\n",
    "            num_folds += 1\n",
    "            padding = target + 2 * overlap - remaining\n",
    "            x = self.pad_tensor(x, padding, side='after')\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            folded = torch.zeros(num_folds, target + 2 * overlap, features).cuda()\n",
    "        else:\n",
    "            folded = torch.zeros(num_folds, target + 2 * overlap, features).cpu()\n",
    "\n",
    "        # Get the values for the folded tensor\n",
    "        for i in range(num_folds):\n",
    "            start = i * (target + overlap)\n",
    "            end = start + target + 2 * overlap\n",
    "            folded[i] = x[:, start:end, :]\n",
    "\n",
    "        return folded\n",
    "\n",
    "    def xfade_and_unfold(self, y, target, overlap):\n",
    "\n",
    "        ''' Applies a crossfade and unfolds into a 1d array.\n",
    "\n",
    "        Args:\n",
    "            y (ndarry)    : Batched sequences of audio samples\n",
    "                            shape=(num_folds, target + 2 * overlap)\n",
    "                            dtype=np.float64\n",
    "            overlap (int) : Timesteps for both xfade and rnn warmup\n",
    "\n",
    "        Return:\n",
    "            (ndarry) : audio samples in a 1d array\n",
    "                       shape=(total_len)\n",
    "                       dtype=np.float64\n",
    "\n",
    "        Details:\n",
    "            y = [[seq1],\n",
    "                 [seq2],\n",
    "                 [seq3]]\n",
    "\n",
    "            Apply a gain envelope at both ends of the sequences\n",
    "\n",
    "            y = [[seq1_in, seq1_target, seq1_out],\n",
    "                 [seq2_in, seq2_target, seq2_out],\n",
    "                 [seq3_in, seq3_target, seq3_out]]\n",
    "\n",
    "            Stagger and add up the groups of samples:\n",
    "\n",
    "            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n",
    "\n",
    "        '''\n",
    "\n",
    "        num_folds, length = y.shape\n",
    "        target = length - 2 * overlap\n",
    "        total_len = num_folds * (target + overlap) + overlap\n",
    "\n",
    "        # Need some silence for the rnn warmup\n",
    "        silence_len = overlap // 2\n",
    "        fade_len = overlap - silence_len\n",
    "        silence = np.zeros((silence_len), dtype=np.float64)\n",
    "\n",
    "        # Equal power crossfade\n",
    "        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n",
    "        fade_in = np.sqrt(0.5 * (1 + t))\n",
    "        fade_out = np.sqrt(0.5 * (1 - t))\n",
    "\n",
    "        # Concat the silence to the fades\n",
    "        fade_in = np.concatenate([silence, fade_in])\n",
    "        fade_out = np.concatenate([fade_out, silence])\n",
    "\n",
    "        # Apply the gain to the overlap samples\n",
    "        y[:, :overlap] *= fade_in\n",
    "        y[:, -overlap:] *= fade_out\n",
    "\n",
    "        unfolded = np.zeros((total_len), dtype=np.float64)\n",
    "\n",
    "        # Loop to add up all the samples\n",
    "        for i in range(num_folds):\n",
    "            start = i * (target + overlap)\n",
    "            end = start + target + 2 * overlap\n",
    "            unfolded[start:end] += y[i]\n",
    "\n",
    "        return unfolded\n",
    "\n",
    "    def get_step(self) :\n",
    "        return self.step.data.item()\n",
    "\n",
    "    def checkpoint(self, model_dir, optimizer) :\n",
    "        k_steps = self.get_step() // 1000\n",
    "        self.save(model_dir.joinpath(\"checkpoint_%dk_steps.pt\" % k_steps), optimizer)\n",
    "\n",
    "    def log(self, path, msg) :\n",
    "        with open(path, 'a') as f:\n",
    "            print(msg, file=f)\n",
    "\n",
    "    def load(self, path, optimizer) :\n",
    "        checkpoint = torch.load(path)\n",
    "        if \"optimizer_state\" in checkpoint:\n",
    "            self.load_state_dict(checkpoint[\"model_state\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        else:\n",
    "            # Backwards compatibility\n",
    "            self.load_state_dict(checkpoint)\n",
    "\n",
    "    def save(self, path, optimizer) :\n",
    "        torch.save({\n",
    "            \"model_state\": self.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def num_params(self, print_out=True):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "        if print_out :\n",
    "            print('Trainable Parameters: %.3fM' % parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pprint\n",
    "\n",
    "class HParams(object):\n",
    "    def __init__(self, **kwargs): self.__dict__.update(kwargs)\n",
    "    def __setitem__(self, key, value): setattr(self, key, value)\n",
    "    def __getitem__(self, key): return getattr(self, key)\n",
    "    def __repr__(self): return pprint.pformat(self.__dict__)\n",
    "\n",
    "    def parse(self, string):\n",
    "        # Overrides hparams from a comma-separated string of name=value pairs\n",
    "        if len(string) > 0:\n",
    "            overrides = [s.split(\"=\") for s in string.split(\",\")]\n",
    "            keys, values = zip(*overrides)\n",
    "            keys = list(map(str.strip, keys))\n",
    "            values = list(map(str.strip, values))\n",
    "            for k in keys:\n",
    "                self.__dict__[k] = ast.literal_eval(values[keys.index(k)])\n",
    "        return self\n",
    "\n",
    "hparams = HParams(\n",
    "        ### Signal Processing (used in both synthesizer and vocoder)\n",
    "        sample_rate = 16000,\n",
    "        n_fft = 800,\n",
    "        num_mels = 80,\n",
    "        hop_size = 200,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\n",
    "        win_size = 800,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\n",
    "        fmin = 55,\n",
    "        min_level_db = -100,\n",
    "        ref_level_db = 20,\n",
    "        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\n",
    "        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\n",
    "        preemphasize = True,\n",
    "\n",
    "        ### Tacotron Text-to-Speech (TTS)\n",
    "        tts_embed_dims = 512,                       # Embedding dimension for the graphemes/phoneme inputs\n",
    "        tts_encoder_dims = 256,\n",
    "        tts_decoder_dims = 128,\n",
    "        tts_postnet_dims = 512,\n",
    "        tts_encoder_K = 5,\n",
    "        tts_lstm_dims = 1024,\n",
    "        tts_postnet_K = 5,\n",
    "        tts_num_highways = 4,\n",
    "        tts_dropout = 0.5,\n",
    "        tts_cleaner_names = [\"english_cleaners\"],\n",
    "        tts_stop_threshold = -3.4,                  # Value below which audio generation ends.\n",
    "                                                    # For example, for a range of [-4, 4], this\n",
    "                                                    # will terminate the sequence at the first\n",
    "                                                    # frame that has all values < -3.4\n",
    "\n",
    "        ### Tacotron Training\n",
    "        tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n",
    "                        (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n",
    "                        (2,  2e-4,  80_000,  12),   #\n",
    "                        (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n",
    "                        (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n",
    "                        (2,  1e-5, 640_000,  12)],  # lr = learning rate\n",
    "\n",
    "        tts_clip_grad_norm = 1.0,                   # clips the gradient norm to prevent explosion - set to None if not needed\n",
    "        tts_eval_interval = 500,                    # Number of steps between model evaluation (sample generation)\n",
    "                                                    # Set to -1 to generate after completing epoch, or 0 to disable\n",
    "\n",
    "        tts_eval_num_samples = 1,                   # Makes this number of samples\n",
    "\n",
    "        ### Data Preprocessing\n",
    "        max_mel_frames = 900,\n",
    "        rescale = True,\n",
    "        rescaling_max = 0.9,\n",
    "        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n",
    "\n",
    "        ### Mel Visualization and Griffin-Lim\n",
    "        signal_normalization = True,\n",
    "        power = 1.5,\n",
    "        griffin_lim_iters = 60,\n",
    "\n",
    "        ### Audio processing options\n",
    "        fmax = 7600,                                # Should not exceed (sample_rate // 2)\n",
    "        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\n",
    "        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\n",
    "        use_lws = False,                            # \"Fast spectrogram phase recovery using local weighted sums\"\n",
    "        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\n",
    "                                                    #               and [0, max_abs_value] if False\n",
    "        trim_silence = True,                        # Use with sample_rate of 16000 for best results\n",
    "\n",
    "        ### SV2TTS\n",
    "        speaker_embedding_size = 256,               # Dimension for the speaker embedding\n",
    "        silence_min_duration_split = 0.4,           # Duration in seconds of a silence for an utterance to be split\n",
    "        utterance_min_duration = 1.6,               # Duration in seconds below which utterances are discarded\n",
    "        )\n",
    "\n",
    "def hparams_debug_string():\n",
    "    return str(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthesizer.hparams import hparams as _syn_hp\n",
    "\n",
    "\n",
    "# Audio settings------------------------------------------------------------------------\n",
    "# Match the values of the synthesizer\n",
    "sample_rate = hparams.sample_rate\n",
    "n_fft = hparams.n_fft\n",
    "num_mels = hparams.num_mels\n",
    "hop_length = hparams.hop_size\n",
    "win_length = hparams.win_size\n",
    "fmin = hparams.fmin\n",
    "min_level_db = hparams.min_level_db\n",
    "ref_level_db = hparams.ref_level_db\n",
    "mel_max_abs_value = hparams.max_abs_value\n",
    "preemphasis = hparams.preemphasis\n",
    "apply_preemphasis = hparams.preemphasize\n",
    "\n",
    "bits = 9                            # bit depth of signal\n",
    "mu_law = True                       # Recommended to suppress noise if using raw bits in hp.voc_mode\n",
    "                                    # below\n",
    "\n",
    "\n",
    "# WAVERNN / VOCODER --------------------------------------------------------------------------------\n",
    "voc_mode = 'RAW'                    # either 'RAW' (softmax on raw bits) or 'MOL' (sample from \n",
    "# mixture of logistics)\n",
    "voc_upsample_factors = (5, 5, 8)    # NB - this needs to correctly factorise hop_length\n",
    "voc_rnn_dims = 512\n",
    "voc_fc_dims = 512\n",
    "voc_compute_dims = 128\n",
    "voc_res_out_dims = 128\n",
    "voc_res_blocks = 10\n",
    "\n",
    "# Training\n",
    "voc_batch_size = 100\n",
    "voc_lr = 1e-4\n",
    "voc_gen_at_checkpoint = 5           # number of samples to generate at each checkpoint\n",
    "voc_pad = 2                         # this will pad the input so that the resnet can 'see' wider \n",
    "                                    # than input length\n",
    "voc_seq_len = hop_length * 5        # must be a multiple of hop_length\n",
    "\n",
    "# Generating / Synthesizing\n",
    "voc_gen_batched = True              # very fast (realtime+) single utterance batched generation\n",
    "voc_target = 8000                   # target number of samples to be generated in each batch entry\n",
    "voc_overlap = 400                   # number of samples for crossfading between batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocoder\n",
    "\n",
    "# from vocoder.models.fatchord_version import WaveRNN\n",
    "# from vocoder import hparams as hp\n",
    "import torch\n",
    "\n",
    "\n",
    "_model = None   # type: WaveRNN\n",
    "\n",
    "def Vload_model(weights_fpath, verbose=True):\n",
    "    global _model, _device\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Building Wave-RNN\")\n",
    "    _model = WaveRNN(\n",
    "        rnn_dims=voc_rnn_dims,\n",
    "        fc_dims=voc_fc_dims,\n",
    "        bits=bits,\n",
    "        pad=voc_pad,\n",
    "        upsample_factors=voc_upsample_factors,\n",
    "        feat_dims=num_mels,\n",
    "        compute_dims=voc_compute_dims,\n",
    "        res_out_dims=voc_res_out_dims,\n",
    "        res_blocks=voc_res_blocks,\n",
    "        hop_length=hop_length,\n",
    "        sample_rate=sample_rate,\n",
    "        mode=voc_mode\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        _model = _model.cuda()\n",
    "        _device = torch.device('cuda')\n",
    "    else:\n",
    "        _device = torch.device('cpu')\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading model weights at %s\" % weights_fpath)\n",
    "    checkpoint = torch.load(weights_fpath, _device)\n",
    "    _model.load_state_dict(checkpoint['model_state'])\n",
    "    _model.eval()\n",
    "\n",
    "\n",
    "def is_loaded():\n",
    "    return _model is not None\n",
    "\n",
    "\n",
    "def Vinfer_waveform(mel, normalize=True,  batched=True, target=8000, overlap=800, \n",
    "                   progress_callback=None):\n",
    "    \"\"\"\n",
    "    Infers the waveform of a mel spectrogram output by the synthesizer (the format must match \n",
    "    that of the synthesizer!)\n",
    "    \n",
    "    :param normalize:  \n",
    "    :param batched: \n",
    "    :param target: \n",
    "    :param overlap: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if _model is None:\n",
    "        raise Exception(\"Please load Wave-RNN in memory before using it\")\n",
    "    \n",
    "    if normalize:\n",
    "        mel = mel / HParams.mel_max_abs_value\n",
    "    mel = torch.from_numpy(mel[None, ...])\n",
    "    wav = _model.generate(mel, batched, target, overlap, HParams.mu_law, progress_callback)\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the encoder model...\n",
      "Loaded encoder \"encoder.pt\" trained to step 1564501\n",
      "Loading the synthesizer model...\n",
      "Synthesizer using device: cpu\n",
      "Loading the vocoder model...\n",
      "Building Wave-RNN\n",
      "Trainable Parameters: 4.481M\n",
      "Loading model weights at Weights\\vocoder.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20700\\3445378155.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_fpath, _device)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20700\\4209753575.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_fpath, _device)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20700\\1305071068.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  audio_mask = np.round(audio_mask).astype(np.bool)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'mels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[230], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m reference_audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples//1320_00000.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test sentence to be synthesized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 51\u001b[0m generated_wav \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_audio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Save the generated audio to a file\u001b[39;00m\n\u001b[0;32m     54\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[230], line 32\u001b[0m, in \u001b[0;36mgenerate_audio\u001b[1;34m(text, reference_audio_path)\u001b[0m\n\u001b[0;32m     29\u001b[0m preprocessed_wav \u001b[38;5;241m=\u001b[39m Epreprocess_wav(reference_audio_path)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Generate the speaker embedding\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mEembed_utterance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Generate the mel spectrogram\u001b[39;00m\n\u001b[0;32m     35\u001b[0m texts \u001b[38;5;241m=\u001b[39m [text]\n",
      "Cell \u001b[1;32mIn[214], line 149\u001b[0m, in \u001b[0;36mEembed_utterance\u001b[1;34m(wav, using_partials, return_partials, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m frames \u001b[38;5;241m=\u001b[39m wav_to_mel_spectrogram(wav)\n\u001b[0;32m    148\u001b[0m frames_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([frames[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mel_slices])\n\u001b[1;32m--> 149\u001b[0m partial_embeds \u001b[38;5;241m=\u001b[39m \u001b[43membed_frames_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Compute the utterance embedding from the partial embeddings\u001b[39;00m\n\u001b[0;32m    152\u001b[0m raw_embed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(partial_embeds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[214], line 57\u001b[0m, in \u001b[0;36membed_frames_batch\u001b[1;34m(frames_batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel was not loaded. Call load_model() before inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(frames_batch)\u001b[38;5;241m.\u001b[39mto(_device)\n\u001b[1;32m---> 57\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embed\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'mels'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# from encoder import inference as encoder\n",
    "# from synthesizer.inference import Synthesizer\n",
    "# from vocoder import inference as vocoder\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define the paths to the pre-trained models\n",
    "encoder_model_fpath = Path(\"Weights//encoder.pt\")\n",
    "synthesizer_model_fpath = Path(\"Weights//synthesizer.pt\")\n",
    "vocoder_model_fpath = Path(\"Weights//vocoder.pt\")\n",
    "\n",
    "# Load the models\n",
    "print(\"Loading the encoder model...\")\n",
    "Eload_model(encoder_model_fpath)\n",
    "\n",
    "print(\"Loading the synthesizer model...\")\n",
    "synthesizer = Synthesizer(synthesizer_model_fpath)\n",
    "\n",
    "print(\"Loading the vocoder model...\")\n",
    "Vload_model(vocoder_model_fpath)\n",
    "\n",
    "# Function to generate audio from text\n",
    "def generate_audio(text, reference_audio_path):\n",
    "    # Preprocess the reference audio file\n",
    "    preprocessed_wav = Epreprocess_wav(reference_audio_path)\n",
    "    \n",
    "    # Generate the speaker embedding\n",
    "    embed = Eembed_utterance(preprocessed_wav)\n",
    "    \n",
    "    # Generate the mel spectrogram\n",
    "    texts = [text]\n",
    "    embeds = [embed]\n",
    "    specs = synthesizer.synthesize_spectrograms(texts, embeds)\n",
    "    spec = specs[0]\n",
    "    \n",
    "    # Generate the waveform\n",
    "    generated_wav = Vinfer_waveform(spec)\n",
    "    \n",
    "    # Post-process the waveform\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    \n",
    "    return generated_wav\n",
    "\n",
    "# Example usage\n",
    "reference_audio_path = \"samples//1320_00000.mp3\"\n",
    "text = \"This is a test sentence to be synthesized.\"\n",
    "generated_wav = generate_audio(text, reference_audio_path)\n",
    "\n",
    "# Save the generated audio to a file\n",
    "output_path = \"output.wav\"\n",
    "sf.write(output_path, generated_wav, samplerate=synthesizer.sample_rate)\n",
    "print(f\"Generated audio saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
